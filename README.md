# Generative AI with Large Language Models (LLM)

## Overview

> Generative AI is a subset of AI, where generative models are used to create content such as text, images etc. This project will focus on generating text content.

This project consists of three generative AI models that can be run on Streamlit applications:
1. **Chatbot:**<br/>LLM based chatbots generate contextually relevant responses to the end-user's queries while considering the entire interaction between chatbot and end-user, and personalising its interaction to meet the end-user's goal.  
2. **Retrieval-Augmented Generation (RAG) Chatbot:**<br/>A Framework that uses LLMs and private data to provide the end-user with more accurate, up-to-date and contextually relevant answers. This is done through retrieval and generation. Relevant information on the end-user's query is found in the private documents and sent to the LLM. The model then generates a response using the end-user's query, the private information received and its own internal knowledge.
3. **Summariser:**<br/>With use of LLMs, concise or specifically structured summaries are generated from longer text documents. The summaries can be generated using a variety of techniques which are showcased in this project.<br/><br/>

## Models
### Chatbot

The end-user can decide how it would like that chatbot to act, this could be the language or the perspective that the chatbot uses when responding, this is referred to as the system prompt. A 'system prompt' is an initial instruction that defines how the model should behave during the conversation. It acts like the chatbotâ€™s 'persona' or 'style' e.g., formal, casual, helpful. For example, asking to the chatbot to respond using a formal tone will result in the chatbot using a professional tone, whereas a casual tone would have resulted in more friendly and informal responses. The end-user can customize this to some extent before the start of the conversation, but it cannot be changed once the conversation begins. The end-user does not need to specify the system prompt, if they do not enter one then the chatbot will act as a helpful assistant. 

Once the end-user enters their prompt it will be stored in the chatbot memory along with the system prompt - the chat history is stored in the session state. The entire conversation, the history in the session state, is sent to the LLM which generates a response. The response is displayed to the end-user by the chatbot and saved to the chat history in the session state. All prompts and responses are stored and sent to the LLM so that the LLM can generate a relevant response to the prompt based on previous prompts and responses. This allows the end-user to ask follow-up questions or reference previous prompts and responses.

The LLM in use is *langchain_openai*'s *ChatOpenAI*. There were two factors that were considered for this LLM - the model and the temperature. Model refers to the OpenAI model that the chatbot is using, by default the model is *'gpt-3.5-turbo'* but the end-user can change this to *'gpt-4o'* or *'gpt-4o-mini'*. Temperature refers to the 'creativeness' of the LLM's generated responses. A low temperature, zero, makes the model produce more consistent and straightforward responses, while a higher temperature, one, makes it more creative and varied in its responses. Generally higher temperatures are used for more creative tasks such as writing a marketing strategy. However, when the temperature is high the chatbot can start hallucinating', which means the model could produce misleading or incorrect responses.
The temperature for this chatbot is set to 0.5 and cannot be changed, although further work to this project could include allowing the end-user to change the model's temperature.

### RAG Chatbot

The purpose of the RAG Chatbot is for the end-user to be able to ask the chatbot questions about their private documents. However, computers cannot understand text, so the text in the uploaded documents needs to be transformed into a form which can be processed. In simple terms, embeddings transform text into a format, numerical vectors, that allows the computer to understand and compare the meaning of words or phrases based on their relationships. The model then performs a similarity search by comparing the embeddings and retrieves the most relevant chunks of text that are similar to the end-user's prompt, see an example in [Figure 1](https://towardsdatascience.com/mastering-customer-segmentation-with-llm-3d9008235f41#3a33). These chunks of text are sent to the LLM along with the end-user's prompt so that the LLM can generate a response using the chunks as the content that it bases its reponse on. <br/><figure><p align="center"><img src='/assets/embedding_sim.webp' style="width: 50%; height: 50%;"><br/><figcaption>*Figure 1: Embeddings Visualisation*</figcaption><p/></figure>
In order to create embeddings the model chunks the data, each of these chunks will be embedded and the similarity searches will be performed on the embedded chunks. Chunking refers to dividing the document into smaller pieces which are know as chunks which allows the LLM to process the contect effectively. If the chunk size is too small the chunks may not provide the model with enough context, however if the chunk is too large the model may not be able to grasp the intricacies of the context. The chunk size in this model is editable which enables the end-user to change the chunk size if they do not feel that the chatbot grasped the context of the document.

The RAG Chatbot can be used for PDF, TXT or DOCX files. Once the end-user has uploaded their document they need to decided what chunk size and the k value that they would like to use for embedding and the similarity search. The chunk size refers to the number of characters in a chunk, and the k value refers to the number of chunks that are retrieved in the similarity search. By setting a higher k value, the model will retrieve more chunks i.e. potential answers, but this may increase processing time. After selecting their chunk size and k value, which have default values of 512 and 3 respectively, the model creates a Chroma storage directory where the embedded data will be stored. The data is split into chunks using the selected chunk size. The chunks are then used to create embedding using *langchain_openai*'s *OpenAIEmbeddings* with the model *'text-embeddind-3-small'*. The embeddings are stored in the Chroma storage Directory. This allows the model to quickly access and compare the document's embeddings when answering queries. Please note that if the uploaded file, chunk size or k value are changed, the Chroma storage directory is cleared and any information about the previous document will be discarded, and the process starts again. 

Once the document has been embedded the end-user can prompt the chatbot. The model will find the k most similar embeddings to the prompt. These embeddings will be sent to the LLM along with the end-user's prompt and the chat history. The chat history is sent to the LLM so that the most relevant response can be generated and to allow the end-user to ask follow up questions or refer to previous answers and responses. The generated response will be displayed by the model, and the prompt and response will be added to the chat history in the session state. Like in the chatbot the LLM in use is *langchain_openai*'s *ChatOpenAI*. By default the model is *'gpt-3.5-turbo'* but the end-user can change this to *'gpt-4o'* or *'gpt-4o-mini'*, and the model temperature is set to zero. As with the chatbot a system prompt is used to tell the LLM how to act, this was done using *langchain*'s *ChatPromptTemplate* function. The system prompt cannot be specified by the end-user in the RAG Chatbot unlike with the previous chatbot. The standard system prompt is "to use the following pieces of context to answer the user's question. If the context provided does not contain information about the question, then use your own knowledge outside the context". This means that if the LLM cannot find that answer in the documents provided then it can use its own knowledge. This can be updated manually in the code so that the LLM only responds using the documents provided.

### Summariser

The data summariser can either summarise text typed into the console or summarise a document. The three document types that it accepts is PDF, TXT or DOCX. The document or text will be loaded and stored to the models memory. There are three options as to how the information can be summarised:
1. Provide a summary for a short document
2. Provide a summary for a long document
3. Provide a summary based on information on Wikipedia
Each of the options mentioned above use *langchain_openai*'s *ChatOpenAI* function. The OpenAI model used by default is *'gpt-3.5-turbo'* but the end-user can change this to *'gpt-4o'* or *'gpt-4o-mini'*.

**More details about each summarisation option:**
1. **Summarise a Short Document:**<br/>When summarising a short document you are given three summarisation options - to provide a concise summary, provide the concise summary in a different language or to summarise the document using specific instructions i.e. include an introduction and conclusion, and summarise all main points using bullet points. For each summarisation option prompts are used to tell the LLM how to summarise the data, this was done using *langchain_core*'s *PromptTemplate* function.<br/><br/>When providing a concise summary the LLM prompt is as follows "You are an expert copywriter with expertise in summarising documents. Write a summary of the following text.".<br/><br/>When converting to a different language the LLM prompt is to "Write a summary of the following text and to translate the summary to *the language given by the end-user*".<br/><br/>When summarising using specific instructions the prompt is entirely the instruction from the end-user.<br/><br/>The information for each option is passed to the LLM using the *stuff* chain approach. The *stuff* chain approach sends all the document's information to the LLM at once, prompting the model to generate a summary based on the entire text. See the visual explanation in Figure 2 below by [Rahul](https://ogre51.medium.com/types-of-chains-in-langchain-823c8878c2e9). Once the summary has been generated by the LLM, it is displayed by the model.<br/><figure><p align="center"><img src='/assets/stuff.webp' style="width: 65%; height: auto;"><br/><figcaption>*Figure 2: Stuff Chain Type Visualisation*</figcaption><p/></figure><br/>
2. **Summarise a Long Document:**<br/>When summarising a longer document you are given two sets of options - the first refers to the instructions sent to the LLM on how to summarise the document, and the second refers to how the information is sent the the LLM and the method used when summarising (chain type). For the first selection, the LLM is asked to provide a concise summary or to summarise the document using specific instructions. The second selections allows the end-user to decide how the information is sent to the LLM - using either the Map Reduce or Refine chain type. These two methods are ways the model handles long documents.<br/><br/>The *Map Reduce* chain type breaks down the document into chunks, summarises each chunk separately and then generate a final summary by summarising all the summaries. See the visual explanation in Figure 3 below by [Rahul](https://ogre51.medium.com/types-of-chains-in-langchain-823c8878c2e9).<br/><figure><p align="center"><img src='/assets/map_refine.webp' style="width: 65%; height: auto;"><br/><figcaption>*Figure 3: Map Reduce Chain Type Visualisation*</figcaption><p/></figure><br/>The *Refine* chain type breaks the document into chunks, creates an initial summary using the first chunk and refines that summary with each subsequent chunk. This process is repeated until a final summary is generated. See the visual explanation in Figure 4 below by [Rahul](https://ogre51.medium.com/types-of-chains-in-langchain-823c8878c2e9). <br/><figure><p align="center"><img src='/assets/refine.webp' style="width: 65%; height: auto;"><br/><figcaption>*Figure 4: Refine Chain Type Visualisation*</figcaption><p/></figure><br/>If the summarisation instructions are to provide a concise summary, then the LLM will generate a standard final summary. Otherwise, if end-user would like the LLM to summarise using specific instructions then the instruction will be added to a final prompt, using *langchain_core*'s *PromptTemplate* function. For example, including an introduction and conclusion and summarising all main points using bullet points. The initial prompt for both the Map Reduce and Refine approach is to write a concise summary. This will be used to generate summaries for all the chunks for the Map Reduce approach, and all the summaries except the final summary for the Refine chain type. The final prompt with the end-user's specific instructions will be used when summarising all the chunk summaries in the Map Reduce approach and to generate the final summary using the last chuck and previous summary for the Refine approach. Once the final summary has been generated by the LLM, it is displayed by the model.<br/><br/>
3. **Summarise Wikipedia Information:** <br/>When summarising information from Wikipedia the end-user needs to provide a question or what they would like information on, and select the reasoning method that they would like the agent to use when summarising the data. The reasoning methods available for this model are *'zero-shot-react-description'* and *'self-ask-with-search'*.<br/><br/>*'zero-shot-react-description'* means that the agent generates a response without asking additional questions, rather relying on what it already knows.<br/><br/>*'self-ask-with-search'* allows the agent to ask itself further questions. This is done so that the agent can gather more context to provide a more informed response.<br/><br/>Information from Wikipedia is accessed using *langchain*'s *WikipediaAPIWrapper* function, and the agent is set up using *langchain*'s *initialize_agent* function. Once the agent has an answer to the end-user's prompt it will be displayed by the model.<br/><br/>

## Streamlit Applications
In order to use the three models from this project, you will need an OpenAI API key and to purchase credits. The OpenAI API key is like a unique password that allows you access to OpenAI's services, and in order to use OpenAI's services you will need to purchase credits. To create an OpenAI API key and purchase credits:
1. Go to the API login on [openai.com](https://platform.openai.com/docs/overview)
2. Set up an account, or login if you already have an account
3. Create a project
4. In the project, go to *API keys* and click *Create new secret key*
5. Review the secret key's permissions
6. Once you have created the key, save it and store it because you will no be able to view it again.
7. Purchase credits.

To run the streamlit apps, use the links provided below and enter your OpenAI API key:
- [Chatbot]()
- [RAG Chatbot]()
- [Summariser]()

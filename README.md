# Generative AI with Large Language Models (LLM)

## Overview

> Generative AI is a subset of AI, where generative models are used to create content such as text, images etc. This project will focus on generating text content.

This project consists of three generative AI models that can be run on Streamlit applications:
1. **Chatbot:**<br/>LLM based chatbots generate contextually relevant responses to the end user's queries while considering the entire interaction between chatbot and end user, and personalising its interaction to meet the end user's goal.  
2. **Retrieval-Augmented Generation (RAG) Chatbot:**<br/>A Framework that uses LLMs and private data to provide the end user with more accurate, up-to-date and contextually relevant answers. This is done through retrieval and generation. Relevant information on the end user's query is found in the private documents and sent to the LLM. The model then generates a response using the end user's query, the private information received and its own internal knowledge.
3. **Summariser:**<br/>With use of LLMs concise or specifically structured summaries are generated from longer text documents. The summaries can be generated using a variety of techniques which are showcased in this project.<br/><br/>

## Models
This section will highlight how each of the models work and have been structured. 
### Chatbot

The end-user can decide how it would like that chatbot to act, this could be the language or the perspective that the chatbot uses when responding, this is referred to as the system prompt. The end-user does not need to specify the system prompt, if they do not enter one then the chatbot will act as a helpful assistant. Once the end-user has entered their first pompt they will not be able to change the system prompt, however they can ask the chatbot to change how they respond within a prompt.

Once the end-user enters their prompt it will be stored in the chatbot memory along with the system prompt - the chat history in the session state. The entire converstion, the history in the session state, are to the LLM which generates a response. The response is display to the user by the chatbot and saved to the history in the session state. All prompts and responses are stored and sent so that the LLM can generate the most relevant response to the prompt based on previous prompts and responses. This allows the end-user to ask follow-up questions or reference previous prompts and responses.

The LLM in use is *langchain_openai*'s *ChatOpenAI*. There were two factors that were considered for this LLM - the model and the temperature. Model refers to the OpenAI model that the chatbot is using, by default the model is *'gpt-3.5-turbo'* but the end-user can change this to *'gpt-4o'* or *'gpt-4o-mini'*. Temperature refers to the 'creativeness' of the LLM's generated responses. Temperature ranges from zero to one, with zero indicating that the responses are straightforward and predictable - you will most likely get the same response if you ask the same prompt multiple times, and one indicating a response that is more creative and that will change if you ask the same prompt more than once. Generally higher temperatures are used for more creative task such as writing a marketing strategy. However, when the temperature is high the chatbot can start hallucinating' which means the model could produce misleading or incorrect results.
The temperature for this chatbot is set to 0.5 and cannot be changed, although further work to this project could include allowing the end-user to change the model's temperature.

### RAG Chatbot

The purpose of the RAG Chatbot is for the end-user to be able to ask the chatbot questions about their private documents. However, computers cannot understand text, so the text in the uploaded documents needs to be transformed into a form which can be processed. Embeddings are real world objects like text, images or videos that are converted into a numerical vectors which computers can process. Once the documents have been embedded similarity searches can be performed. The model will find pieces of text that are similar to the end-user's prompt by finding vectors that are similar to each other, see an example in [Figure 1](https://towardsdatascience.com/mastering-customer-segmentation-with-llm-3d9008235f41#3a33). <br/><figure><p align="center"><img src='/assets/embedding_sim.webp' style="width: 50%; height: 50%;"><br/><figcaption>*Figure 1: Embeddings Visualisation*</figcaption><p/></figure>
In order to create embeddings the model needs to break the data up into chunks, each of these chunks will be embedded and the similarity searches will be done on the embedded chunks. If the chunk size is too small there is a chance that the chunk will nprovide the model with enough context, however if the chunk is too large the model may not be able to grasp the intricacies of the context.

The RAG Chatbot can be used for pdf, txt or docx files. Once the end-user has uploaded their document they need to decided what chuck size and the k value that they would like to use for embedding and the similarity search. The chunk size refers to the number of characters in a chunk, and the k value refers to the number of chunks that are similary to the end-user's prompt found in the similarity search which are sent to the LLM.

Once the end-user has uploaded their data and selected their chunk size and k, which have default vaules of 512 and 3 respectively, the model created a Chroma storage directory where the embedded data will be stored. The data is split into chunks based of the selected chunk size

User adds their document, if they would like to specify specifies the chuck size (the number of characters in a chunk - the more letters in a chuck the more information is sent to the LLM which could result in a more accurate answer. If the chuck size is too small you may lose information. EXPLAIN more) and the number of similar items that must be retrieved and sent to the LLM (k).
The model created a Chroma storage directory where the embeddings will be stored. The chunks are used to create embedding using *langchain_openai*'s *OpenAIEmbeddings* using the model *'text-embeddind-3-small'*. The embeddings are added to a collection and stored in the Chroma storage Directory. Please note that if the uploaded file, chunk size or k are changed, the Chroma storage directory is cleared and any information about the previous document will be discarded. The embedding process will start again. 

Once the document has been uploaded the end-user can prompt the chatbot. The model will find the k most similar embeddings to the prompt. These embeddings will be sent to the LLM along with the end-user's prompt and the chat history. The chat history is sent to the LLM so that the most relevant response can be generated and to allow the end-user to ask follow up questions or refer to previous answers and responses. The generated response will be displayed by the model, and the prompt and response will be added to the chat history in the session state. Like in the chatbot the LLM in use is *langchain_openai*'s *ChatOpenAI*. By default the model is *'gpt-3.5-turbo'* but the end-user can change this to *'gpt-4o'* or *'gpt-4o-mini'*, and the model temperatue is set to zero. As with the chatbot a system prompt is used to tell the LLM how to act, this was done using *langchain*'s *ChatPromptTemplate* function. The system prompt cannot be specified by the end-user in the Rag Chatbot unlike with the previous chatbot. The standard system prompt is "to use the following pieces of context to answer the user's question. If the context provided does not contain information about the question, then use your own knowledge outside the context". This means that if the LLM cannot find that answer in the documents provided then it can use its own knowledge. This can be updated manually in the code so that the LLM only responds using the documents provided.

### Summariser

The data summariser can either summarise text typed into the console or summarise a document. The three document types that it accepts is pdf, txt or docx. The document or text will be loaded and stored to the models memory. There are three options:
1. Provide a summary for a short document
2. Provide a summary for a long document
3. Provide a summary based on information on Wikipedia
Each of the options mentioned above use *langchain_openai*'s *ChatOpenAI* function. The OpenAI model used by default is *'gpt-3.5-turbo'* but the end-user can change this to *'gpt-4o'* or *'gpt-4o-mini'*.

**More details about each summarisation option:**
1. **Summarise a Short Document:**<br/>When summarising a short document you are given three summarisation options - to provide a concise summary, provide the concise summary in a different language or to summarise the document using specific instructions i.e. include an introduction and conclusion, and summarise all main points using bullet points. For each summarisation option prompts are used to tell the LLM how to summarise the data, this was done using *langchain_core*'s *PromptTemplate* function.<br/><br/>When providing a concise summary the LLM prompt is as follows "You are an expert copywriter with expertise in summarising documents. Write a summary of the following text.".<br/><br/>When converting to a different language the LLM prompt is to "Write a summary of the following text and to translate the summary to *the language given by the end user*".<br/><br/>When summarising using specific instructions the prompt is entirely the instruction from the end-user.<br/><br/>The information for each option is passed to the LLM using the *stuff* chain approach, i.e. the entire document is summarised in a single prompt. See the visual explaination in Figure 2 below by [Rahul](https://ogre51.medium.com/types-of-chains-in-langchain-823c8878c2e9). Once the summary has been generated by the LLM, it is displayed by the model.<br/><figure><p align="center"><img src='/assets/stuff.webp' style="width: 65%; height: auto;"><br/><figcaption>*Figure 2: Stuff Chain Type Visualisation*</figcaption><p/></figure><br/>
2. **Summarise a Long Document:**<br/>When summarising a longer document you are given two sets of options - the first refers to the instructions sent to the LLM on how to summarise the document, and the second refers to how the information is sent the the LLM (chain type). For the first option the LLM is asked to provide a concise summary or to summarise the document using specific instructions. The second option sends the information to the LLM using either the Map Reduce or Refine chain type.<br/><br/>The Map Reduce chain type chunks the document(s) and instructs the LLM to summarise the chunks of the document separately and then to provide a final summary by summarising the summarises of each chunk. See the visual explaination in Figure 3 below by [Rahul](https://ogre51.medium.com/types-of-chains-in-langchain-823c8878c2e9).<br/><figure><p align="center"><img src='/assets/map_refine.webp' style="width: 65%; height: auto;"><br/><figcaption>*Figure 3: Map Reduce Chain Type Visualisation*</figcaption><p/></figure><br/>The Refine chain type chunks the document(s) and instructs the LLM to create a summary of the first chunk, the to create another summary using the next chunk and the previous summary. The final summary is produced by summarising the last chunk of the document and the previous summary. See the visual explaination in Figure 4 below by [Rahul](https://ogre51.medium.com/types-of-chains-in-langchain-823c8878c2e9). <br/><figure><p align="center"><img src='/assets/refine.webp' style="width: 65%; height: auto;"><br/><figcaption>*Figure 4: Refine Chain Type Visualisation*</figcaption><p/></figure><br/>If the summarisiation instructions are to provide a concise summary, then the LLM will create the final summary as it created the previous summaries. Otherwise, if you like to LLM to summarise using sepcific instriuctions then the instruction will be added to a final prompt, using *langchain_core*'s *PromptTemplate* function. The inital prompt for both the Map Reduce and Refine approach is to write a concise summary. This will be used to generate summaries for all the chunks for the Map Reduce approach, and all the summaries except the final summary for the Refine chain type. The final prompt with the end-user's specific instructions will be used when summarising all the chunk summaries in the Map Reduce approach and to generate the final summary using the last chuck and previous summary for the Refine approach. Once the final summary has been generated by the LLM, it is displayed by the model.<br/><br/>
3. **Summarise Wikipedia Information:**<br/>When summarising information from Wikipedia the end-user needs to provide a question or what they would like information on, and select the reasoning method that they would like the agent to use when summarising the data. The reasoning methods available for this model are *'zero-shot-react-description'* and *'self-ask-with-search'*.<br/><br/>*'zero-shot-react-description'* refers to the reasoning method where the agent does not ask additional questions but rather generates a response based on the information that it has.<br/><br/>*'self-ask-with-search'* refers to the reasoning method where the agent will ask itself additional questions about the end-users topic and based on its findings to these questions generate a more informed response.<br/><br/>Information from Wikipedia is accessed using *langchain*'s *WikipediaAPIWrapper* function, and the agent is set up using *langchain*'s *initialize_agent* function. Once the agent has an answer to the end-user's prompt it will be displayed by the model.<br/><br/>

## Setting Up Your Environment / Prerequisites
